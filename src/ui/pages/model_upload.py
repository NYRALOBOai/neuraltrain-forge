#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
P√°gina de upload de modelos do NeuralTrain Forge.
"""

import streamlit as st
import os
import tempfile
from pathlib import Path
from typing import Optional, Dict, Any
import json


def render():
    """Renderiza a p√°gina de upload de modelos."""
    
    # Header da p√°gina
    st.markdown("# üì§ Upload de Modelos")
    st.markdown("Carregue modelos locais ou baixe do HuggingFace Hub")
    
    # Tabs para diferentes m√©todos de upload
    tab1, tab2, tab3 = st.tabs(["üìÅ Upload Local", "ü§ó HuggingFace Hub", "üìã Modelos Carregados"])
    
    with tab1:
        render_local_upload()
    
    with tab2:
        render_huggingface_download()
    
    with tab3:
        render_loaded_models()


def render_local_upload():
    """Renderiza interface para upload de modelos locais."""
    
    st.markdown("## üìÅ Upload de Arquivo Local")
    st.markdown("Carregue modelos nos formatos .gguf, .bin ou .safetensors")
    
    # Informa√ß√µes sobre formatos suportados
    with st.expander("‚ÑπÔ∏è Formatos Suportados"):
        st.markdown("""
        **Formatos aceitos:**
        - **.gguf**: Modelos quantizados (llama.cpp)
        - **.bin**: Modelos PyTorch tradicionais
        - **.safetensors**: Formato seguro do HuggingFace
        
        **Tamanho m√°ximo:** 1GB por arquivo
        
        **Estrutura recomendada:**
        - Para modelos completos: inclua config.json e tokenizer
        - Para modelos √∫nicos: apenas o arquivo do modelo
        """)
    
    # Upload de arquivo
    uploaded_file = st.file_uploader(
        "Selecione o arquivo do modelo",
        type=['gguf', 'bin', 'safetensors'],
        help="Arraste e solte ou clique para selecionar"
    )
    
    if uploaded_file is not None:
        # Mostra informa√ß√µes do arquivo
        file_details = {
            "Nome": uploaded_file.name,
            "Tamanho": f"{uploaded_file.size / (1024*1024):.2f} MB",
            "Tipo": uploaded_file.type or "Desconhecido"
        }
        
        st.markdown("### üìÑ Informa√ß√µes do Arquivo")
        for key, value in file_details.items():
            st.info(f"**{key}:** {value}")
        
        # Configura√ß√µes do modelo
        st.markdown("### ‚öôÔ∏è Configura√ß√µes")
        
        col1, col2 = st.columns(2)
        
        with col1:
            model_name = st.text_input(
                "Nome do modelo",
                value=Path(uploaded_file.name).stem,
                help="Nome que ser√° usado para identificar o modelo"
            )
        
        with col2:
            model_type = st.selectbox(
                "Tipo do modelo",
                ["Auto-detectar", "LLaMA", "Mistral", "GPT", "Outro"],
                help="Tipo do modelo para configura√ß√µes espec√≠ficas"
            )
        
        # Descri√ß√£o opcional
        description = st.text_area(
            "Descri√ß√£o (opcional)",
            placeholder="Descreva o modelo, sua origem, caracter√≠sticas especiais...",
            help="Informa√ß√µes adicionais sobre o modelo"
        )
        
        # Valida√ß√£o do nome
        if model_name:
            if not model_name.replace('_', '').replace('-', '').isalnum():
                st.error("‚ùå Nome deve conter apenas letras, n√∫meros, h√≠fens e underscores")
            elif len(model_name) < 3:
                st.error("‚ùå Nome deve ter pelo menos 3 caracteres")
            else:
                # Bot√£o de upload
                if st.button("üì§ Carregar Modelo", type="primary", use_container_width=True):
                    upload_local_model(uploaded_file, model_name, model_type, description)
        else:
            st.warning("‚ö†Ô∏è Digite um nome para o modelo")


def render_huggingface_download():
    """Renderiza interface para download do HuggingFace."""
    
    st.markdown("## ü§ó Download do HuggingFace Hub")
    st.markdown("Baixe modelos diretamente do reposit√≥rio HuggingFace")
    
    # Informa√ß√µes sobre o HuggingFace
    with st.expander("‚ÑπÔ∏è Como usar"):
        st.markdown("""
        **Exemplos de modelos populares:**
        - `microsoft/DialoGPT-medium`: Modelo conversacional
        - `gpt2`: GPT-2 original
        - `distilgpt2`: Vers√£o menor do GPT-2
        - `EleutherAI/gpt-neo-1.3B`: GPT-Neo 1.3B
        
        **Formato do ID:** `organizacao/nome-do-modelo`
        
        **Nota:** Alguns modelos podem ser grandes (v√°rios GB)
        """)
    
    # Modelos populares
    st.markdown("### üåü Modelos Populares")
    
    popular_models = [
        {
            "id": "microsoft/DialoGPT-medium",
            "name": "DialoGPT Medium",
            "description": "Modelo conversacional baseado em GPT-2",
            "size": "~1.5GB"
        },
        {
            "id": "gpt2",
            "name": "GPT-2",
            "description": "Modelo original GPT-2 da OpenAI",
            "size": "~500MB"
        },
        {
            "id": "distilgpt2",
            "name": "DistilGPT-2",
            "description": "Vers√£o destilada e menor do GPT-2",
            "size": "~350MB"
        }
    ]
    
    cols = st.columns(len(popular_models))
    
    for i, model in enumerate(popular_models):
        with cols[i]:
            with st.container():
                st.markdown(f"**{model['name']}**")
                st.caption(model['description'])
                st.caption(f"Tamanho: {model['size']}")
                
                if st.button(f"üì• Baixar", key=f"download_{i}", use_container_width=True):
                    download_huggingface_model(model['id'], model['name'])
    
    st.markdown("---")
    
    # Download personalizado
    st.markdown("### üîç Download Personalizado")
    
    col1, col2 = st.columns(2)
    
    with col1:
        model_id = st.text_input(
            "ID do modelo",
            placeholder="ex: microsoft/DialoGPT-medium",
            help="ID completo do modelo no HuggingFace Hub"
        )
    
    with col2:
        local_name = st.text_input(
            "Nome local (opcional)",
            placeholder="Nome para salvar localmente",
            help="Se vazio, usar√° o nome original"
        )
    
    # Configura√ß√µes avan√ßadas
    with st.expander("‚öôÔ∏è Configura√ß√µes Avan√ßadas"):
        col1, col2 = st.columns(2)
        
        with col1:
            config_name = st.text_input(
                "Configura√ß√£o espec√≠fica",
                placeholder="ex: default",
                help="Nome da configura√ß√£o espec√≠fica (opcional)"
            )
        
        with col2:
            revision = st.text_input(
                "Revis√£o/Branch",
                placeholder="ex: main",
                help="Branch ou commit espec√≠fico (opcional)"
            )
        
        use_auth_token = st.checkbox(
            "Usar token de autentica√ß√£o",
            help="Para modelos privados ou com restri√ß√µes"
        )
        
        if use_auth_token:
            auth_token = st.text_input(
                "Token HuggingFace",
                type="password",
                help="Seu token de acesso do HuggingFace"
            )
    
    # Valida√ß√£o e download
    if model_id:
        if '/' not in model_id:
            st.warning("‚ö†Ô∏è ID deve estar no formato 'organizacao/modelo'")
        else:
            if st.button("ü§ó Baixar do HuggingFace", type="primary", use_container_width=True):
                download_huggingface_model(
                    model_id, 
                    local_name or model_id.replace('/', '_'),
                    config_name or None,
                    revision or None
                )


def render_loaded_models():
    """Renderiza lista de modelos carregados."""
    
    st.markdown("## üìã Modelos Carregados")
    st.markdown("Gerencie os modelos dispon√≠veis no sistema")
    
    # Bot√£o para atualizar lista
    col1, col2, col3 = st.columns([1, 1, 2])
    
    with col1:
        if st.button("üîÑ Atualizar", use_container_width=True):
            st.rerun()
    
    with col2:
        if st.button("üóëÔ∏è Limpar Cache", use_container_width=True):
            clear_model_cache()
    
    # Lista de modelos (placeholder)
    # Em uma implementa√ß√£o real, isso viria do ModelManager
    models = get_loaded_models_list()
    
    if not models:
        st.info("üì≠ Nenhum modelo carregado ainda")
        st.markdown("Use as abas acima para carregar seu primeiro modelo!")
    else:
        # Filtros
        st.markdown("### üîç Filtros")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            filter_type = st.selectbox(
                "Tipo",
                ["Todos", "LLaMA", "Mistral", "GPT", "Outro"]
            )
        
        with col2:
            filter_source = st.selectbox(
                "Origem",
                ["Todas", "Local", "HuggingFace"]
            )
        
        with col3:
            sort_by = st.selectbox(
                "Ordenar por",
                ["Nome", "Data", "Tamanho", "Tipo"]
            )
        
        # Lista de modelos
        st.markdown("### üìö Modelos Dispon√≠veis")
        
        for i, model in enumerate(models):
            render_model_card(model, i)


def render_model_card(model: Dict[str, Any], index: int):
    """Renderiza card de um modelo."""
    
    with st.container():
        col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
        
        with col1:
            st.markdown(f"**ü§ñ {model['name']}**")
            st.caption(f"Tipo: {model['type']} | Origem: {model['source']}")
            if model.get('description'):
                st.caption(model['description'])
        
        with col2:
            st.metric("Tamanho", model['size'])
        
        with col3:
            status_color = "üü¢" if model['status'] == 'loaded' else "üîµ"
            st.markdown(f"{status_color} {model['status']}")
        
        with col4:
            # Menu de a√ß√µes
            action = st.selectbox(
                "A√ß√µes",
                ["Selecionar", "Carregar", "Detalhes", "Remover"],
                key=f"action_{index}",
                label_visibility="collapsed"
            )
            
            if action == "Carregar":
                load_model(model['name'])
            elif action == "Detalhes":
                show_model_details(model)
            elif action == "Remover":
                remove_model(model['name'])
        
        st.markdown("---")


def upload_local_model(uploaded_file, model_name: str, model_type: str, description: str):
    """Processa upload de modelo local."""
    
    try:
        with st.spinner("üì§ Carregando modelo..."):
            # Cria arquivo tempor√°rio
            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            # Aqui seria chamado o ModelManager para processar o upload
            # success, message = model_manager.upload_model(tmp_file_path, model_name)
            
            # Simula√ß√£o de sucesso
            success = True
            message = f"Modelo {model_name} carregado com sucesso!"
            
            # Remove arquivo tempor√°rio
            os.unlink(tmp_file_path)
            
            if success:
                st.success(f"‚úÖ {message}")
                
                # Mostra informa√ß√µes do modelo carregado
                st.balloons()
                
                # Adiciona ao hist√≥rico
                add_to_upload_history("local", model_name, uploaded_file.size)
                
            else:
                st.error(f"‚ùå {message}")
                
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar modelo: {str(e)}")


def download_huggingface_model(model_id: str, local_name: str, config_name: Optional[str] = None, revision: Optional[str] = None):
    """Processa download do HuggingFace."""
    
    try:
        with st.spinner(f"ü§ó Baixando {model_id}..."):
            # Aqui seria chamado o ModelManager para baixar do HuggingFace
            # success, message = model_manager.download_from_huggingface(model_id, local_name)
            
            # Simula√ß√£o de sucesso
            success = True
            message = f"Modelo {model_id} baixado com sucesso!"
            
            if success:
                st.success(f"‚úÖ {message}")
                st.balloons()
                
                # Adiciona ao hist√≥rico
                add_to_upload_history("huggingface", local_name, "N/A")
                
            else:
                st.error(f"‚ùå {message}")
                
    except Exception as e:
        st.error(f"‚ùå Erro ao baixar modelo: {str(e)}")


def get_loaded_models_list():
    """Retorna lista de modelos carregados (placeholder)."""
    
    # Em uma implementa√ß√£o real, isso viria do ModelManager
    return [
        {
            "name": "LLaMA-7B-Chat",
            "type": "LLaMA",
            "source": "HuggingFace",
            "size": "13.5GB",
            "status": "loaded",
            "description": "Modelo conversacional baseado em LLaMA"
        },
        {
            "name": "Mistral-7B-Instruct",
            "type": "Mistral", 
            "source": "Local",
            "size": "14.2GB",
            "status": "available",
            "description": "Modelo de instru√ß√£o Mistral"
        },
        {
            "name": "Custom-GPT2",
            "type": "GPT",
            "source": "Local",
            "size": "548MB",
            "status": "loaded",
            "description": "Modelo GPT-2 personalizado"
        }
    ]


def load_model(model_name: str):
    """Carrega modelo na mem√≥ria."""
    
    with st.spinner(f"üîÑ Carregando {model_name}..."):
        # Aqui seria chamado o ModelManager
        # success, message, model = model_manager.load_model(model_name)
        
        # Simula√ß√£o
        success = True
        message = f"Modelo {model_name} carregado na mem√≥ria"
        
        if success:
            st.success(f"‚úÖ {message}")
        else:
            st.error(f"‚ùå {message}")


def show_model_details(model: Dict[str, Any]):
    """Mostra detalhes do modelo."""
    
    with st.expander(f"üìã Detalhes - {model['name']}", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Informa√ß√µes B√°sicas:**")
            st.text(f"Nome: {model['name']}")
            st.text(f"Tipo: {model['type']}")
            st.text(f"Origem: {model['source']}")
            st.text(f"Tamanho: {model['size']}")
            st.text(f"Status: {model['status']}")
        
        with col2:
            st.markdown("**Configura√ß√µes:**")
            st.text("Arquitetura: Transformer")
            st.text("Precis√£o: FP16")
            st.text("Contexto: 2048 tokens")
            st.text("Vocabul√°rio: 32000")
        
        if model.get('description'):
            st.markdown("**Descri√ß√£o:**")
            st.text(model['description'])


def remove_model(model_name: str):
    """Remove modelo do sistema."""
    
    if st.button(f"‚ö†Ô∏è Confirmar remo√ß√£o de {model_name}", type="secondary"):
        with st.spinner(f"üóëÔ∏è Removendo {model_name}..."):
            # Aqui seria chamado o ModelManager
            # success, message = model_manager.delete_model(model_name)
            
            # Simula√ß√£o
            success = True
            message = f"Modelo {model_name} removido com sucesso"
            
            if success:
                st.success(f"‚úÖ {message}")
                st.rerun()
            else:
                st.error(f"‚ùå {message}")


def clear_model_cache():
    """Limpa cache de modelos."""
    
    with st.spinner("üßπ Limpando cache..."):
        # Aqui seria implementada a limpeza do cache
        st.success("‚úÖ Cache limpo com sucesso!")


def add_to_upload_history(source: str, name: str, size):
    """Adiciona entrada ao hist√≥rico de uploads."""
    
    if 'upload_history' not in st.session_state:
        st.session_state.upload_history = []
    
    entry = {
        'timestamp': str(pd.Timestamp.now()),
        'source': source,
        'name': name,
        'size': size
    }
    
    st.session_state.upload_history.append(entry)
    
    # Mant√©m apenas os √∫ltimos 10
    if len(st.session_state.upload_history) > 10:
        st.session_state.upload_history = st.session_state.upload_history[-10:]

